{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio # Scipy input and output\n",
    "import scipy.ndimage \n",
    "from skimage.transform import rotate \n",
    "import spectral # Module for processing hyperspectral image data.\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# keras imports \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.set_image_data_format('channels_last')\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.logging.set_verbosity(tf.logging.INFO) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.config.list_physical_devices('XLA_GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.compat.v1.ConfigProto() \n",
    "#jit_level = tf.OptimizerOptions.ON_1\n",
    "#config.graph_options.optimizer_options.global_jit_level = jit_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3396343787379313168\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1332100190061096984\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5890289054352689126\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11329617920\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12682156088672292557\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: e527:00:00.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "           inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "           device_count = {'CPU' : 1, 'GPU' : 1},log_device_placement=True)\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "#config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 6},allow_soft_placement=True ) \n",
    "#sess = tf.compat.v1.Session(config=config) \n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "#tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading  and Preproccesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_dataset(dataset):\n",
    "    \"\"\"load dataset parameters from config.json\"\"\"\n",
    "    \n",
    "    with open('./model_config.json') as f:\n",
    "        config = json.loads(f.read())\n",
    "        params = config[dataset]\n",
    "        data = sio.loadmat(params['img_path'])[params['img']]\n",
    "        labels = sio.loadmat(params['gt_path'])[params['gt']]\n",
    "        num_classes = params['num_classes']\n",
    "        target_names = params['target_names']\n",
    "        \n",
    "    return data,labels,num_classes,target_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, num_components=75):\n",
    "    \"\"\"apply pca to X and return new_X\"\"\"\n",
    "    \n",
    "    new_X = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=num_components, whiten=True)\n",
    "    new_X = pca.fit_transform(new_X)\n",
    "    new_X = np.reshape(new_X, (X.shape[0],X.shape[1], num_components))\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(X, margin=2):\n",
    "    \"\"\"apply zero padding to X with margin\"\"\"\n",
    "    \n",
    "    new_X = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    new_X[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(X, y, window_size=5, remove_zero_labels = True):\n",
    "    \"\"\"create patch from image. suppose the image has the shape (w,h,c) then the patch shape is\n",
    "    (w*h,window_size,window_size,c)\"\"\"\n",
    "    \n",
    "    margin = int((window_size - 1) / 2)\n",
    "    zero_padded_X = pad_with_zeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patches_data = np.zeros((X.shape[0] * X.shape[1], window_size, window_size, X.shape[2]))\n",
    "    patchs_labels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patch_index = 0\n",
    "    for r in range(margin, zero_padded_X.shape[0] - margin):\n",
    "        for c in range(margin, zero_padded_X.shape[1] - margin):\n",
    "            patch = zero_padded_X[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patches_data[patch_index, :, :, :] = patch\n",
    "            patchs_labels[patch_index] = y[r-margin, c-margin]\n",
    "            patch_index = patch_index + 1\n",
    "    if remove_zero_labels:\n",
    "        patches_data = patches_data[patchs_labels>0,:,:,:]\n",
    "        patchs_labels = patchs_labels[patchs_labels>0]\n",
    "        patchs_labels -= 1\n",
    "    return patches_data, patchs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(X, y, test_ratio=0.10):\n",
    "    \"\"\"split dataset into train set and test set with test_ratio\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_weak_classes(X, y):\n",
    "    \"\"\"\"balance the dataset by prforming oversample of weak classes (making each class have close labels_counts)\"\"\"\n",
    "    unique_labels, labels_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    print(unique_labels.shape)\n",
    "    print(unique_labels)\n",
    "    print(labels_counts.shape)\n",
    "    print(labels_counts)\n",
    "    max_count = np.max(labels_counts)\n",
    "    labels_inverse_ratios = max_count / labels_counts  \n",
    "    #print(labels_inverse_ratios)\n",
    "    # repeat for every label and concat\n",
    "    print(labels_inverse_ratios)\n",
    "    new_X = X[y == unique_labels[0], :, :, :].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    new_Y = y[y == unique_labels[0]].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(unique_labels[1:], labels_inverse_ratios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        new_X = np.concatenate((new_X, cX))\n",
    "        new_Y = np.concatenate((new_Y, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(new_Y.shape[0])\n",
    "    new_X = new_X[rand_perm, :, :, :]\n",
    "    new_Y = new_Y[rand_perm]\n",
    "    unique_labels, labels_counts = np.unique(new_Y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train):\n",
    "    \"\"\"augment the data by taking each patch and randomly performing \n",
    "    a flip(up/down or right/left) or a rotation\"\"\"\n",
    "    \n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"PaviaU\" # Indian_pines or PaviaU or or Salinas  . check config.json\n",
    "window_size = 5\n",
    "num_pca_components = 30\n",
    "test_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing steps\n",
    "<img src=\"./images/1.png\" alt=\"diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "(9,)\n",
      "[ 4973 13987  1574  2298  1009  3772   998  2761   710]\n",
      "[ 2.81258798  1.          8.886277    6.08659704 13.86223984  3.70811241\n",
      " 14.01503006  5.06591815 19.7       ]\n"
     ]
    }
   ],
   "source": [
    "X, y , num_classes , target_names = load_dataset(dataset)\n",
    "\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "\n",
    "X_patches, y_patches = create_patches(X, y, window_size=window_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test_set(X_patches, y_patches, test_ratio)\n",
    "\n",
    "X_train, y_train = oversample_weak_classes(X_train, y_train)\n",
    "\n",
    "X_train = augment_data(X_train)\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) # convert class labels to on-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(optimizer=SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)) :\n",
    "    \n",
    "    \n",
    "    input_shape= X_train[0].shape # Define the input shape \n",
    "    C1 = 3*num_pca_components # number of filters\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(C1, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(3*C1, (3, 3), activation='relu'))\n",
    "#    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6*num_pca_components, activation='relu'))\n",
    "#    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "128051/128051 [==============================] - 11s 82us/step - loss: 1.1890 - accuracy: 0.6644\n",
      "Epoch 2/3\n",
      "128051/128051 [==============================] - 10s 82us/step - loss: 0.3371 - accuracy: 0.9211\n",
      "Epoch 3/3\n",
      "128051/128051 [==============================] - 10s 81us/step - loss: 0.1953 - accuracy: 0.9446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff294684f28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.fit(X_train, y_train, batch_size=2**6, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "m=X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "with tf.device('device:XLA_CPU:0'):\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.MirroredStrategy( devices=[\"device:XLA_GPU:0\"])\n",
    "#model007 = define_model()\n",
    "#with mirrored_strategy.scope():\n",
    "#    model007.fit(X_train, y_train, batch_size=32, epochs=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have already trained the model\n",
    "\n",
    "model69 = define_model()\n",
    "with tf.device('device:XLA_GPU:0'):\n",
    "    model69.fit(X_train, y_train, batch_size=32, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already trained the model\n",
    "model = define_model(optimizer=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07))\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    test_Loss =  score[0]*100\n",
    "    test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, test_Loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(classification, confusion, test_loss, test_accuracy) :\n",
    "    \n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "\n",
    "    report = '{} Test loss (%)'.format(test_loss)\n",
    "    report += '\\n'\n",
    "    report += '{} Test accuracy (%)'.format(test_accuracy)\n",
    "    report += '\\n'\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(classification)\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(confusion)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report):\n",
    "    file_name = 'report_{}_{}_{}_{}.txt'.format(dataset,window_size,num_pca_components,test_ratio)\n",
    "    with open('./reports/{}'.format(file_name), 'w') as report_file:\n",
    "        report_file.write(report)\n",
    "        print(\"\\n\\nReport saved to {}\".format(file_name))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pretrained model make predictions and print the results into a reportclassification, confusion, test_loss, test_accuracy = generate_report(X_test,y_test)\n",
    "\n",
    "classification, confusion, test_Loss, test_accuracy = generate_report(X_test,y_test)\n",
    "report = show_report(classification, confusion, test_Loss, test_accuracy)\n",
    "save_report(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "dataset=\"PaviaU\"\n",
    "X, y , num_classes,target_names= load_dataset(dataset)\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pridected_image():\n",
    "    \"\"\"generate the predicted image\"\"\"\n",
    "    outputs = np.zeros((height,width))\n",
    "\n",
    "    for i in range(height-patch_size+1):\n",
    "        for j in range(width-patch_size+1):\n",
    "            target = y[int(i+patch_size/2), int(j+patch_size/2)]\n",
    "            if target == 0 :\n",
    "                continue\n",
    "            else :\n",
    "                image_patch=get_patch(X,i,j)\n",
    "                #print (image_patch.shape)\n",
    "                X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],image_patch.shape[2]).astype('float32')                                   \n",
    "                prediction = (model.predict_classes(X_test_image))                         \n",
    "                outputs[int(i+patch_size/2)][int(j+patch_size/2)] = prediction+1\n",
    "    return outputs.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Ground Truth Image\n",
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Predicted image\n",
    "outputs=generate_pridected_image()\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
