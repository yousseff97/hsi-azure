{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Image Classification\n",
    "insert small description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from random import shuffle\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io as sio # Scipy input and output\n",
    "import scipy.ndimage \n",
    "from skimage.transform import rotate \n",
    "import spectral # Module for processing hyperspectral image data.\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# keras imports \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Conv3D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading  and Preproccesing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_dataset(dataset):\n",
    "    \"\"\"load dataset parameters from config.json\"\"\"\n",
    "    \n",
    "    with open('./config.json') as f:\n",
    "        config = json.loads(f.read())\n",
    "        params = config[dataset]\n",
    "        data = sio.loadmat(params['img_path'])[params['img']]\n",
    "        labels = sio.loadmat(params['gt_path'])[params['gt']]\n",
    "        num_classes = params['num_classes']\n",
    "        target_names = params['target_names']\n",
    "        \n",
    "    return data,labels,num_classes,target_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, num_components=75):\n",
    "    \"\"\"apply pca to X and return new_X\"\"\"\n",
    "    \n",
    "    new_X = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=num_components, whiten=True)\n",
    "    new_X = pca.fit_transform(new_X)\n",
    "    new_X = np.reshape(new_X, (X.shape[0],X.shape[1], num_components))\n",
    "    return new_X, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_zeros(X, margin=2):\n",
    "    \"\"\"apply zero padding to X with margin\"\"\"\n",
    "    \n",
    "    new_X = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    new_X[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return new_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(X, y, window_size=5, remove_zero_labels = True):\n",
    "    \"\"\"create patch from image. suppose the image has the shape (w,h,c) then the patch shape is\n",
    "    (w*h,window_size,window_size,c)\"\"\"\n",
    "    \n",
    "    margin = int((window_size - 1) / 2)\n",
    "    zero_padded_X = pad_with_zeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patches_data = np.zeros((X.shape[0] * X.shape[1], window_size, window_size, X.shape[2]))\n",
    "    patchs_labels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patch_index = 0\n",
    "    for r in range(margin, zero_padded_X.shape[0] - margin):\n",
    "        for c in range(margin, zero_padded_X.shape[1] - margin):\n",
    "            patch = zero_padded_X[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patches_data[patch_index, :, :, :] = patch\n",
    "            patchs_labels[patch_index] = y[r-margin, c-margin]\n",
    "            patch_index = patch_index + 1\n",
    "    if remove_zero_labels:\n",
    "        patches_data = patches_data[patchs_labels>0,:,:,:]\n",
    "        patchs_labels = patchs_labels[patchs_labels>0]\n",
    "        patchs_labels -= 1\n",
    "    return patches_data, patchs_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_set(X, y, test_ratio=0.10):\n",
    "    \"\"\"split dataset into train set and test set with test_ratio\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=345,\n",
    "                                                        stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_weak_classes(X, y):\n",
    "    \"\"\"\"balance the dataset by prforming oversample of weak classes (making each class have close labels_counts)\"\"\"\n",
    "    unique_labels, labels_counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    print(unique_labels.shape)\n",
    "    print(unique_labels)\n",
    "    print(labels_counts.shape)\n",
    "    print(labels_counts)\n",
    "    max_count = np.max(labels_counts)\n",
    "    labels_inverse_ratios = max_count / labels_counts  \n",
    "    #print(labels_inverse_ratios)\n",
    "    # repeat for every label and concat\n",
    "    print(labels_inverse_ratios)\n",
    "    new_X = X[y == unique_labels[0], :, :, :].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    new_Y = y[y == unique_labels[0]].repeat(round(labels_inverse_ratios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(unique_labels[1:], labels_inverse_ratios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        new_X = np.concatenate((new_X, cX))\n",
    "        new_Y = np.concatenate((new_Y, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(new_Y.shape[0])\n",
    "    new_X = new_X[rand_perm, :, :, :]\n",
    "    new_Y = new_Y[rand_perm]\n",
    "    unique_labels, labels_counts = np.unique(new_Y, return_counts=True)\n",
    "    \n",
    "#     print(unique_labels.shape)\n",
    "#     print(unique_labels)\n",
    "#     print(labels_counts.shape)\n",
    "#     print(labels_counts)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train):\n",
    "    \"\"\"augment the data by taking each patch and randomly performing \n",
    "    a flip(up/down or right/left) or a rotation\"\"\"\n",
    "    \n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "        patch2 = flipped_patch\n",
    "        X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "dataset = \"Indian_pines\" # Indian_pines or PaviaU or or Salinas  . check config.json\n",
    "window_size = 5\n",
    "num_pca_components = 30\n",
    "test_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproccesing steps\n",
    "<!---img src=\"./images/1.png\" alt=\"diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial (145, 145, 200)\n",
      "PCA (145, 145, 30)\n",
      "Patches (10249, 5, 5, 30) \n",
      "(10249,)\n",
      "Split (7686, 5, 5, 30)\n",
      "(16,)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      "(16,)\n",
      "[  35 1071  622  178  362  547   21  358   15  729 1841  445  154  949\n",
      "  289   70]\n",
      "[ 52.6          1.71895425   2.95980707  10.34269663   5.08563536\n",
      "   3.36563071  87.66666667   5.1424581  122.73333333   2.52537723\n",
      "   1.           4.13707865  11.95454545   1.93993678   6.37024221\n",
      "  26.3       ]\n",
      "Oversample (29685, 5, 5, 30)\n",
      "Augmentaion (29685, 5, 5, 30) \n"
     ]
    }
   ],
   "source": [
    "X, y , num_classes , target_names = load_dataset(dataset)\n",
    "print(\"Initial {}\".format(X.shape))\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "print(\"PCA {}\".format(X.shape))\n",
    "X_patches, y_patches = create_patches(X, y, window_size=window_size)\n",
    "print(\"Patches {} \".format(X_patches.shape))\n",
    "print(y_patches.shape)\n",
    "X_train, X_test, y_train, y_test = split_train_test_set(X_patches, y_patches, test_ratio)\n",
    "print(\"Split {}\".format(X_train.shape))\n",
    "X_train, y_train = oversample_weak_classes(X_train, y_train)\n",
    "print(\"Oversample {}\".format(X_train.shape))\n",
    "X_train = augment_data(X_train)\n",
    "print(\"Augmentaion {} \".format(X_train.shape))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train) # convert class labels to on-hot encoding\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model() :\n",
    "    \n",
    "    filters_number = 90;\n",
    "    input_shape= X_train[0].shape # Define the input shape \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters_number,(24,3,3),activation='relu',input_shape=input_shape))\n",
    "    \n",
    "    \n",
    "    input_shape= X_train[0].shape # Define the input shape \n",
    "    C1 = 3*num_pca_components # number of filters\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(C1, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(3*C1, (3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6*num_pca_components, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./saved_models/model_{}_{}_{}_{}.h5'.format(dataset,window_size,num_pca_components,test_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report (X_test,y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
    "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "    test_Loss =  score[0]*100\n",
    "    test_accuracy = score[1]*100\n",
    "    \n",
    "    return classification, confusion, test_Loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_report(classification, confusion, test_loss, test_accuracy) :\n",
    "    \n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "\n",
    "    report = '{} Test loss (%)'.format(test_loss)\n",
    "    report += '\\n'\n",
    "    report += '{} Test accuracy (%)'.format(test_accuracy)\n",
    "    report += '\\n'\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(classification)\n",
    "    report += '\\n'\n",
    "    report += '{}'.format(confusion)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report):\n",
    "    file_name = 'report_{}_{}_{}_{}.txt'.format(dataset,window_size,num_pca_components,test_ratio)\n",
    "    with open('./reports/{}'.format(file_name), 'w') as report_file:\n",
    "        report_file.write(report)\n",
    "        print(\"\\n\\nReport saved to {}\".format(file_name))\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch(data,height_index,width_index):\n",
    "    #transpose_array = data.transpose((2,0,1))\n",
    "    #print transpose_array.shape\n",
    "    height_slice = slice(height_index, height_index+patch_size)\n",
    "    width_slice = slice(width_index, width_index+patch_size)\n",
    "    patch = data[height_slice, width_slice, :]\n",
    "    \n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the pretrained model make predictions and print the results into a reportclassification, confusion, test_loss, test_accuracy = generate_report(X_test,y_test)\n",
    "\n",
    "classification, confusion, test_Loss, test_accuracy = generate_report(X_test,y_test)\n",
    "report = show_report(classification, confusion, test_Loss, test_accuracy)\n",
    "save_report(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the original image\n",
    "dataset=\"Indian_pines\"\n",
    "X, y , num_classes,target_names= load_dataset(dataset)\n",
    "X,pca = apply_pca(X,num_pca_components)\n",
    "height = y.shape[0]\n",
    "width = y.shape[1]\n",
    "patch_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pridected_image():\n",
    "    \"\"\"generate the predicted image\"\"\"\n",
    "    outputs = np.zeros((height,width))\n",
    "\n",
    "    for i in range(height-patch_size+1):\n",
    "        for j in range(width-patch_size+1):\n",
    "            target = y[int(i+patch_size/2), int(j+patch_size/2)]\n",
    "            if target == 0 :\n",
    "                continue\n",
    "            else :\n",
    "                image_patch=get_patch(X,i,j)\n",
    "                #print (image_patch.shape)\n",
    "                X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1],image_patch.shape[2]).astype('float32')                                   \n",
    "                prediction = (model.predict_classes(X_test_image))                         \n",
    "                outputs[int(i+patch_size/2)][int(j+patch_size/2)] = prediction+1\n",
    "    return outputs.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Ground Truth Image\n",
    "ground_truth = spectral.imshow(classes = y,figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the Predicted image\n",
    "outputs=generate_pridected_image()\n",
    "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3",
   "language": "python",
   "name": "mypython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
